{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07dfc8da",
   "metadata": {},
   "source": [
    "# Execution num 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "from ns3gym import ns3env\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import tqdm\n",
    "import subprocess\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from agents.ddpg.agent import Agent, Config\n",
    "from agents.ddpg.model import Actor\n",
    "from agents.teacher import Teacher, EnvWrapper\n",
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "scenario = \"convergence\"\n",
    "\n",
    "simTime = 60 # seconds\n",
    "stepTime = 0.01  # seconds\n",
    "history_length = 300\n",
    "\n",
    "EPISODE_COUNT = 15\n",
    "steps_per_ep = int(simTime/stepTime)\n",
    "\n",
    "sim_args = {\n",
    "    \"simTime\": simTime,\n",
    "    \"envStepTime\": stepTime,\n",
    "    \"historyLength\": history_length,\n",
    "    \"agentType\": Agent.TYPE,\n",
    "    \"scenario\": \"convergence\",\n",
    "    \"nWifi\": 30,\n",
    "}\n",
    "\n",
    "print(\"Steps per episode:\", steps_per_ep)\n",
    "\n",
    "threads_no = 1\n",
    "env = EnvWrapper(threads_no, **sim_args)\n",
    "\n",
    "#%%\n",
    "env.reset()\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "\n",
    "print(\"Observation space shape:\", ob_space)\n",
    "print(\"Action space shape:\", ac_space)\n",
    "\n",
    "assert ob_space is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d83dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "teacher = Teacher(env, 1, Preprocessor(False))\n",
    "\n",
    "lr_actor = 4e-4\n",
    "lr_critic = 4e-3\n",
    "\n",
    "agent_count = 30 # Number of agents to train\n",
    "agents = []    # List to store the agents\n",
    "\n",
    "for i in range(agent_count):\n",
    "    config = Config(buffer_size=4*steps_per_ep*threads_no, batch_size=32, gamma=0.7, tau=1e-3, lr_actor=lr_actor, lr_critic=lr_critic, update_every=1)\n",
    "    agent = Agent(history_length, action_size=1, config=config, actor_layers=[8, 128, 16], critic_layers=[8,128,16])\n",
    "    agents.append(agent)\n",
    "    \n",
    "# Test the model\n",
    "hyperparams = {**config.__dict__, **sim_args}\n",
    "tags = [\"Rew: normalized speed\",\n",
    "        f\"{Agent.NAME}\",\n",
    "        sim_args['scenario'],\n",
    "        f\"Actor: {lr_actor}\",\n",
    "        f\"Critic: {lr_critic}\",\n",
    "        f\"Instances: {threads_no}\",\n",
    "        f\"Station count: {sim_args['nWifi']}\",\n",
    "        *[f\"{key}: {sim_args[key]}\" for key in list(sim_args)[:3]]]\n",
    "# agent.save()\n",
    "logger = teacher.train(agents, EPISODE_COUNT,\n",
    "                        simTime=simTime,\n",
    "                        stepTime=stepTime,\n",
    "                        history_length=history_length,\n",
    "                        send_logs=True,\n",
    "                        experimental=True,\n",
    "                        tags=tags,\n",
    "                        parameters=hyperparams)\n",
    "# logger = teacher.eval(agents,\n",
    "#                         simTime=simTime,\n",
    "#                         stepTime=stepTime,\n",
    "#                         history_length=history_length,\n",
    "#                         tags=tags,\n",
    "#                         parameters=hyperparams)\n",
    "# agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433b685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from ns3gym import ns3env\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import tqdm\n",
    "import subprocess\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from agents.ddpg.agent import Agent, Config\n",
    "from agents.ddpg.model import Actor\n",
    "from agents.teacher import Teacher, EnvWrapper\n",
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37879c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "scenario = \"basic\"\n",
    "\n",
    "simTime = 60 # seconds\n",
    "stepTime = 0.01  # seconds\n",
    "history_length = 300\n",
    "\n",
    "EPISODE_COUNT = 15\n",
    "steps_per_ep = int(simTime/stepTime)\n",
    "\n",
    "sim_args = {\n",
    "    \"simTime\": simTime,\n",
    "    \"envStepTime\": stepTime,\n",
    "    \"historyLength\": history_length,\n",
    "    \"agentType\": Agent.TYPE,\n",
    "    \"scenario\": \"basic\",\n",
    "    \"nWifi\": 5,\n",
    "}\n",
    "\n",
    "print(\"Steps per episode:\", steps_per_ep)\n",
    "\n",
    "threads_no = 1\n",
    "env = EnvWrapper(threads_no, **sim_args)\n",
    "\n",
    "#%%\n",
    "env.reset()\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "\n",
    "print(\"Observation space shape:\", ob_space)\n",
    "print(\"Action space shape:\", ac_space)\n",
    "\n",
    "assert ob_space is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "teacher = Teacher(env, 1, Preprocessor(False))\n",
    "\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-3\n",
    "\n",
    "agent_count = 5 # Number of agents to train\n",
    "agents = []    # List to store the agents\n",
    "\n",
    "for i in range(agent_count):\n",
    "    config = Config(buffer_size=4*steps_per_ep*threads_no, batch_size=32, gamma=0.7, tau=1e-3, lr_actor=lr_actor, lr_critic=lr_critic, update_every=1)\n",
    "    agent = Agent(history_length, action_size=1, config=config, actor_layers=[8, 128, 16], critic_layers=[8,128,16])\n",
    "    agents.append(agent)\n",
    "    \n",
    "# Test the model\n",
    "hyperparams = {**config.__dict__, **sim_args}\n",
    "tags = [\"Rew: normalized speed\",\n",
    "        f\"{Agent.NAME}\",\n",
    "        sim_args['scenario'],\n",
    "        f\"Actor: {lr_actor}\",\n",
    "        f\"Critic: {lr_critic}\",\n",
    "        f\"Instances: {threads_no}\",\n",
    "        f\"Station count: {sim_args['nWifi']}\",\n",
    "        *[f\"{key}: {sim_args[key]}\" for key in list(sim_args)[:3]]]\n",
    "# agent.save()\n",
    "logger = teacher.train(agents, EPISODE_COUNT,\n",
    "                        simTime=simTime,\n",
    "                        stepTime=stepTime,\n",
    "                        history_length=history_length,\n",
    "                        send_logs=True,\n",
    "                        experimental=True,\n",
    "                        tags=tags,\n",
    "                        parameters=hyperparams)\n",
    "# logger = teacher.eval(agents,\n",
    "#                         simTime=simTime,\n",
    "#                         stepTime=stepTime,\n",
    "#                         history_length=history_length,\n",
    "#                         tags=tags,\n",
    "#                         parameters=hyperparams)\n",
    "# agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc22e14",
   "metadata": {},
   "source": [
    "# Execution num 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from ns3gym import ns3env\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import tqdm\n",
    "import subprocess\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from agents.ddpg.agent import Agent, Config\n",
    "from agents.ddpg.model import Actor\n",
    "from agents.teacher import Teacher, EnvWrapper\n",
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244223d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "scenario = \"basic\"\n",
    "\n",
    "simTime = 60 # seconds\n",
    "stepTime = 0.01  # seconds\n",
    "history_length = 300\n",
    "\n",
    "EPISODE_COUNT = 15\n",
    "steps_per_ep = int(simTime/stepTime)\n",
    "\n",
    "sim_args = {\n",
    "    \"simTime\": simTime,\n",
    "    \"envStepTime\": stepTime,\n",
    "    \"historyLength\": history_length,\n",
    "    \"agentType\": Agent.TYPE,\n",
    "    \"scenario\": \"basic\",\n",
    "    \"nWifi\": 5,\n",
    "}\n",
    "\n",
    "print(\"Steps per episode:\", steps_per_ep)\n",
    "\n",
    "threads_no = 1\n",
    "env = EnvWrapper(threads_no, **sim_args)\n",
    "\n",
    "#%%\n",
    "env.reset()\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "\n",
    "print(\"Observation space shape:\", ob_space)\n",
    "print(\"Action space shape:\", ac_space)\n",
    "\n",
    "assert ob_space is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa76fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "teacher = Teacher(env, 1, Preprocessor(False))\n",
    "\n",
    "lr_actor = 4e-4\n",
    "lr_critic = 4e-3\n",
    "\n",
    "agent_count = 5 # Number of agents to train\n",
    "agents = []    # List to store the agents\n",
    "\n",
    "for i in range(agent_count):\n",
    "    config = Config(buffer_size=4*steps_per_ep*threads_no, batch_size=32, gamma=0.7, tau=1e-3, lr_actor=lr_actor, lr_critic=lr_critic, update_every=1)\n",
    "    agent = Agent(history_length, action_size=1, config=config, actor_layers=[8, 128, 16], critic_layers=[8,128,16])\n",
    "    agents.append(agent)\n",
    "    \n",
    "# Test the model\n",
    "hyperparams = {**config.__dict__, **sim_args}\n",
    "tags = [\"Rew: normalized speed\",\n",
    "        f\"{Agent.NAME}\",\n",
    "        sim_args['scenario'],\n",
    "        f\"Actor: {lr_actor}\",\n",
    "        f\"Critic: {lr_critic}\",\n",
    "        f\"Instances: {threads_no}\",\n",
    "        f\"Station count: {sim_args['nWifi']}\",\n",
    "        *[f\"{key}: {sim_args[key]}\" for key in list(sim_args)[:3]]]\n",
    "# agent.save()\n",
    "logger = teacher.train(agents, EPISODE_COUNT,\n",
    "                        simTime=simTime,\n",
    "                        stepTime=stepTime,\n",
    "                        history_length=history_length,\n",
    "                        send_logs=True,\n",
    "                        experimental=True,\n",
    "                        tags=tags,\n",
    "                        parameters=hyperparams)\n",
    "# logger = teacher.eval(agents,\n",
    "#                         simTime=simTime,\n",
    "#                         stepTime=stepTime,\n",
    "#                         history_length=history_length,\n",
    "#                         tags=tags,\n",
    "#                         parameters=hyperparams)\n",
    "# agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbcead",
   "metadata": {},
   "source": [
    "# Execution num 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from ns3gym import ns3env\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import tqdm\n",
    "import subprocess\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from agents.ddpg.agent import Agent, Config\n",
    "from agents.ddpg.model import Actor\n",
    "from agents.teacher import Teacher, EnvWrapper\n",
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ae1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "scenario = \"basic\"\n",
    "\n",
    "simTime = 60 # seconds\n",
    "stepTime = 0.01  # seconds\n",
    "history_length = 300\n",
    "\n",
    "EPISODE_COUNT = 15\n",
    "steps_per_ep = int(simTime/stepTime)\n",
    "\n",
    "sim_args = {\n",
    "    \"simTime\": simTime,\n",
    "    \"envStepTime\": stepTime,\n",
    "    \"historyLength\": history_length,\n",
    "    \"agentType\": Agent.TYPE,\n",
    "    \"scenario\": \"basic\",\n",
    "    \"nWifi\": 5,\n",
    "}\n",
    "\n",
    "print(\"Steps per episode:\", steps_per_ep)\n",
    "\n",
    "threads_no = 1\n",
    "env = EnvWrapper(threads_no, **sim_args)\n",
    "\n",
    "#%%\n",
    "env.reset()\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "\n",
    "print(\"Observation space shape:\", ob_space)\n",
    "print(\"Action space shape:\", ac_space)\n",
    "\n",
    "assert ob_space is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc998500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "teacher = Teacher(env, 1, Preprocessor(False))\n",
    "\n",
    "lr_actor = 4e-4\n",
    "lr_critic = 4e-3\n",
    "\n",
    "agent_count = 5 # Number of agents to train\n",
    "agents = []    # List to store the agents\n",
    "\n",
    "for i in range(agent_count):\n",
    "    config = Config(buffer_size=4*steps_per_ep*threads_no, batch_size=32, gamma=0.7, tau=1e-3, lr_actor=lr_actor, lr_critic=lr_critic, update_every=1)\n",
    "    agent = Agent(history_length, action_size=1, config=config, actor_layers=[8, 128, 16], critic_layers=[8,128,16])\n",
    "    agents.append(agent)\n",
    "    \n",
    "# Test the model\n",
    "hyperparams = {**config.__dict__, **sim_args}\n",
    "tags = [\"Rew: normalized speed\",\n",
    "        f\"{Agent.NAME}\",\n",
    "        sim_args['scenario'],\n",
    "        f\"Actor: {lr_actor}\",\n",
    "        f\"Critic: {lr_critic}\",\n",
    "        f\"Instances: {threads_no}\",\n",
    "        f\"Station count: {sim_args['nWifi']}\",\n",
    "        *[f\"{key}: {sim_args[key]}\" for key in list(sim_args)[:3]]]\n",
    "# agent.save()\n",
    "logger = teacher.train(agents, EPISODE_COUNT,\n",
    "                        simTime=simTime,\n",
    "                        stepTime=stepTime,\n",
    "                        history_length=history_length,\n",
    "                        send_logs=True,\n",
    "                        experimental=True,\n",
    "                        tags=tags,\n",
    "                        parameters=hyperparams)\n",
    "# logger = teacher.eval(agents,\n",
    "#                         simTime=simTime,\n",
    "#                         stepTime=stepTime,\n",
    "#                         history_length=history_length,\n",
    "#                         tags=tags,\n",
    "#                         parameters=hyperparams)\n",
    "# agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5472c3c",
   "metadata": {},
   "source": [
    "# para obter a pcol de cada uma das 5 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f255cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from ns3gym import ns3env\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import tqdm\n",
    "import subprocess\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from agents.ddpg.agent import Agent, Config\n",
    "from agents.ddpg.model import Actor\n",
    "from agents.teacher import Teacher, EnvWrapper\n",
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "scenario = \"basic\"\n",
    "\n",
    "simTime = 60 # seconds\n",
    "stepTime = 0.01  # seconds\n",
    "history_length = 300\n",
    "\n",
    "EPISODE_COUNT = 15\n",
    "steps_per_ep = int(simTime/stepTime)\n",
    "\n",
    "sim_args = {\n",
    "    \"simTime\": simTime,\n",
    "    \"envStepTime\": stepTime,\n",
    "    \"historyLength\": history_length,\n",
    "    \"agentType\": Agent.TYPE,\n",
    "    \"scenario\": \"basic\",\n",
    "    \"nWifi\": 5,\n",
    "}\n",
    "\n",
    "print(\"Steps per episode:\", steps_per_ep)\n",
    "\n",
    "threads_no = 1\n",
    "env = EnvWrapper(threads_no, **sim_args)\n",
    "\n",
    "#%%\n",
    "env.reset()\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "\n",
    "print(\"Observation space shape:\", ob_space)\n",
    "print(\"Action space shape:\", ac_space)\n",
    "\n",
    "assert ob_space is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8479c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "teacher = Teacher(env, 1, Preprocessor(False))\n",
    "\n",
    "lr_actor = 4e-4\n",
    "lr_critic = 4e-3\n",
    "\n",
    "agent_count = 5 # Number of agents to train\n",
    "agents = []    # List to store the agents\n",
    "\n",
    "for i in range(agent_count):\n",
    "    config = Config(buffer_size=4*steps_per_ep*threads_no, batch_size=32, gamma=0.7, tau=1e-3, lr_actor=lr_actor, lr_critic=lr_critic, update_every=1)\n",
    "    agent = Agent(history_length, action_size=1, config=config, actor_layers=[8, 128, 16], critic_layers=[8,128,16])\n",
    "    agents.append(agent)\n",
    "    \n",
    "# Test the model\n",
    "hyperparams = {**config.__dict__, **sim_args}\n",
    "tags = [\"Rew: normalized speed\",\n",
    "        f\"{Agent.NAME}\",\n",
    "        sim_args['scenario'],\n",
    "        f\"Actor: {lr_actor}\",\n",
    "        f\"Critic: {lr_critic}\",\n",
    "        f\"Instances: {threads_no}\",\n",
    "        f\"Station count: {sim_args['nWifi']}\",\n",
    "        *[f\"{key}: {sim_args[key]}\" for key in list(sim_args)[:3]]]\n",
    "# agent.save()\n",
    "logger = teacher.train(agents, EPISODE_COUNT,\n",
    "                        simTime=simTime,\n",
    "                        stepTime=stepTime,\n",
    "                        history_length=history_length,\n",
    "                        send_logs=True,\n",
    "                        experimental=True,\n",
    "                        tags=tags,\n",
    "                        parameters=hyperparams)\n",
    "# logger = teacher.eval(agents,\n",
    "#                         simTime=simTime,\n",
    "#                         stepTime=stepTime,\n",
    "#                         history_length=history_length,\n",
    "#                         tags=tags,\n",
    "#                         parameters=hyperparams)\n",
    "# agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227eaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
